{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Jason Olefson Project 2 Part 1 Deep Learning</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Imports</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.callbacks import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>GPU Check</h1>\n",
    "<h3>IMPORTANT In this project, I used my GPU (NVIDIA RTX 4080) to train my model. Because of this, you may need to configure the first few blocks on this file to run on your device. Thank you.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for GPU availability\n",
    "# print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "# # Check if TensorFlow is using GPU for cuDNN-enabled operations\n",
    "# print(\"Is TensorFlow using GPU?: \", tf.test.is_built_with_cuda())\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"GPU is available and being used.\")\n",
    "else:\n",
    "    print(\"No GPU detected, using CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Save Model Function</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, model_dir='saved_model', model_name='project2_part1_model.h5'):\n",
    "    os.makedirs(model_dir, exist_ok=True) # create dir if no exist\n",
    "    model_path = os.path.join(model_dir, model_name)\n",
    "    model.save(model_path)\n",
    "    print(f\"Model saved to: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Data Prep</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data directory path\n",
    "data_dir = \"./Data/\"\n",
    "\n",
    "data_frames = [] # empty list to hold DataFrames\n",
    "\n",
    "# Combine data into single DataFrame\n",
    "# loop over each subfolder\n",
    "for subfolder in os.listdir(data_dir):\n",
    "    subfolder_path = os.path.join(data_dir, subfolder)\n",
    "    # check if directory\n",
    "    if os.path.isdir(subfolder_path):\n",
    "        # loop over all csv\n",
    "        for file in os.listdir(subfolder_path):\n",
    "            if file.endswith(\".csv\"):\n",
    "                file_path = os.path.join(subfolder_path, file)\n",
    "                df = pd.read_csv(file_path, header=None) #read csv\n",
    "                data_frames.append(df) # add Datafram to list\n",
    "combined_data = pd.concat(data_frames, ignore_index=True) #combine into single DataFrame\n",
    "combined_data.columns = [\"Baby_ID\", \"Heart_Rate\", \"Respiratory_Rate\", \"Oxygen_Saturation\", \"Pain_Level\"] # rename columns for clarity\n",
    "combined_data = combined_data[combined_data[\"Pain_Level\"] != \"#\"] # remove rows with \"#\" in the 4th column\n",
    "# Remove the pesky typo\n",
    "combined_data[\"Heart_Rate\"] = pd.to_numeric(combined_data[\"Heart_Rate\"], errors='coerce')\n",
    "combined_data[\"Respiratory_Rate\"] = pd.to_numeric(combined_data[\"Respiratory_Rate\"], errors='coerce')\n",
    "combined_data[\"Oxygen_Saturation\"] = pd.to_numeric(combined_data[\"Oxygen_Saturation\"], errors='coerce')\n",
    "combined_data.dropna(subset=[\"Heart_Rate\", \"Respiratory_Rate\", \"Oxygen_Saturation\"], inplace=True)\n",
    "combined_data.head() # for clarity (display first few columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Shuffle/Split Dataset</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = shuffle(combined_data, random_state=42) # shuffle dataset\n",
    "train, temp = train_test_split(combined_data, test_size=0.1, random_state=42) # split to training/temp sets (90% training, 10% temp)\n",
    "validation, test = train_test_split(temp, test_size=0.5, random_state=42) # split temp set into validation/test sets (10% of total each)\n",
    "\n",
    "# separate features/labels for each set\n",
    "X_train = train[[\"Heart_Rate\", \"Respiratory_Rate\", \"Oxygen_Saturation\"]]\n",
    "y_train = train[\"Pain_Level\"]\n",
    "X_val = validation[[\"Heart_Rate\", \"Respiratory_Rate\", \"Oxygen_Saturation\"]]\n",
    "y_val = validation[\"Pain_Level\"]\n",
    "X_test = test[[\"Heart_Rate\", \"Respiratory_Rate\", \"Oxygen_Saturation\"]]\n",
    "y_test = test[\"Pain_Level\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Normalization</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "# Normalization for feature columns\n",
    "X_train = scaler.fit_transform(train[[\"Heart_Rate\", \"Respiratory_Rate\", \"Oxygen_Saturation\"]])\n",
    "X_val = scaler.transform(validation[[\"Heart_Rate\", \"Respiratory_Rate\", \"Oxygen_Saturation\"]])\n",
    "X_test = scaler.transform(test[[\"Heart_Rate\", \"Respiratory_Rate\", \"Oxygen_Saturation\"]])\n",
    "# Convert normalized arrays back to DataFrames\n",
    "X_train = pd.DataFrame(X_train, columns=[\"Heart_Rate\", \"Respiratory_Rate\", \"Oxygen_Saturation\"])\n",
    "X_val = pd.DataFrame(X_val, columns=[\"Heart_Rate\", \"Respiratory_Rate\", \"Oxygen_Saturation\"])\n",
    "X_test = pd.DataFrame(X_test, columns=[\"Heart_Rate\", \"Respiratory_Rate\", \"Oxygen_Saturation\"])\n",
    "# Label extraction\n",
    "y_train = train[\"Pain_Level\"].reset_index(drop=True)\n",
    "y_val = validation[\"Pain_Level\"].reset_index(drop=True)\n",
    "y_test = test[\"Pain_Level\"].reset_index(drop=True)\n",
    "\n",
    "X_train.head() # for clarity (display first few columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to float32/reshape to (samples, timesteps, features)\n",
    "X_train = X_train.values.astype('float32').reshape(-1, 3, 1)\n",
    "X_val = X_val.values.astype('float32').reshape(-1, 3, 1)\n",
    "X_test = X_test.values.astype('float32').reshape(-1, 3, 1)\n",
    "\n",
    "# Convert to int32\n",
    "y_train = y_train.astype('int32')\n",
    "y_val = y_val.astype('int32')\n",
    "y_test = y_test.astype('int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Learning Rate Adjuster</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_adjuster = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.05,\n",
    "    patience=5,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Build GRU Model</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Architecture\n",
    "model = Sequential([\n",
    "    GRU(16, return_sequences=True, input_shape=(X_train.shape[1], 1), kernel_regularizer=l2(0.001)),\n",
    "    Dropout(0.2),\n",
    "    GRU(16, return_sequences=True, kernel_regularizer=l2(0.001)),\n",
    "    Dropout(0.2),\n",
    "    GRU(16, return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    GRU(16, return_sequences=False, kernel_regularizer=l2(0.001)),\n",
    "    Dropout(0.2),\n",
    "    Dense(3, activation='softmax') # 3 unit output layer (for 3 classes) / softmax activation\n",
    "])\n",
    "\n",
    "# Compile\n",
    "model.compile(optimizer=Adam(learning_rate=0.01), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary() # display summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Training</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epochs/Batch size (adjust as needed)\n",
    "epochs = 60\n",
    "batch_size = 32\n",
    "\n",
    "# Train model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    verbose=1,\n",
    "    callbacks=[lr_adjuster]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Save Model</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the below code to save the current model (if happy)\n",
    "# save_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Plot Training and Valid Performance</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training/Validation Accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Training/Validation Loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Loss Over Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Report</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Data Preprocessing and Training Techniques Used</h2>\n",
    "<ul>\n",
    "    <li>Normalization</li>\n",
    "    <li>Dropout</li>\n",
    "    <li>Regularization</li>\n",
    "    <li>Train-Validation Splitting</li>\n",
    "    <li>Shuffling</li>\n",
    "</ul>\n",
    "<h2>Below are the Training and Validation Results in Figures</h2>\n",
    "<img src=\"Images/Part 1 Model Accuracy.png\">\n",
    "<img src=\"Images/Part 1 Loss Over Epochs.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
